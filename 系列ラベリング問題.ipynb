{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 系列ラベリング問題とは？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* データの系列$ \\bf{x}=\\{x_1,x_2,...,x_n\\} $を入力として、ラベルの系列$ \\bf{y}=\\{y_1,y_2,...,y_n\\} $を出力とする問題のこと\n",
    "* $ y_iとy_{i+1} $にはある種の関係があり、$ y_1,y_2,...,y_n $を独立に考えることができない\n",
    "* 「私は秋田犬が大好き。」\n",
    "* 「私」「は」「秋田」「犬」「が」「大好き」「。」が$ x_1,x_2,...,x_7 $に対応\n",
    "* 「名詞」「助詞」「名詞」「名詞」「助詞」「名詞」「記号」が$ y_1,y_2,...,y_7 $に対応\n",
    "* 単語分割→各文字に対してその文字の後に単語区切りの記号を入れる(ラベル1)か入れない(ラベル0)かのラベルを付与する問題とすることで、系列ラベリング問題と見なせる\n",
    "* 固有表現抽出→あるカテゴリーを持つ単語や複合語に対してラベルを付与する\n",
    "  * 人名(岸田文雄を例に挙げる)→人名の開始のラベル(岸田)：B-Personを付与、人名内のラベル(文雄)：I-Personを付与"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ディープラーニング出現前の系列ラベリング問題の解法\n",
    "#### まず解決すべき課題\n",
    "* 個々のデータ$ x_i $をどのようなベクトルで表現するか→対象のタスクを解くのに有効そうな特徴を次元に対応させてベクトル化する\n",
    "* $ \\bf{y} $ = $ f(\\bf{x}) $となるような関数$ f $をどのように構築するか（学習）\n",
    "* 関数$ f $を利用して入力系列$ \\bf{x} $から$ f(\\bf{x}) $をどのように計算するか（推論）\n",
    "\n",
    "#### HMM(Hidden Markov Model、隠れマルコフモデル)\n",
    "* 有限次元オートマトンに確率を付与したモデル\n",
    "* 品詞タガ－(「S(開始記号)」「E(終了記号)」「名詞」「動詞」「副詞」「助詞」)を用いる\n",
    "* SからEまでに出現する単語$ w_0,w_1,...,w_m $が観測され、これらがどのような状態をたどってきたかを推定し、各単語$ w_i $に品詞を付与する\n",
    "* 状態$ S_i $から状態$ S_j $に移動する確率$ p(S_j|S_i) $とその移動する際に単語$ w $を出力する確率$ p(w|S_i) $を設定→EMアルゴリズムによって学習を行う\n",
    "* $ p(w|S_i) $が最大となるような状態の列を推定したい→組み合わせ最適化問題となる\n",
    "\n",
    "#### CRF(Conditional Random Field、条件付確率場)\n",
    "* 対数線形モデルの一種\n",
    "* $ p(\\bf{y}|\\bf{x})=\\frac{1}{Z_{x,w}} e^{w ・\\phi(\\bf{x}, \\bf{y})} $　$ w $：重みベクトル\n",
    "* $ Z_{x,w}=\\sum_{y} e^{w ・ \\phi(\\bf{x},\\bf{y})} $\n",
    "* $ \\phi(\\bf{x}, \\bf{y}) $：$ \\bf{x}, \\bf{y} $を入力として、K種類の素性、つまりK次元ベクトルを出力する関数\n",
    "* $ \\phi(\\bf{x}, \\bf{y})=(\\phi_{1}(\\bf{x}, \\bf{y}), \\phi_{2}(\\bf{x}, \\bf{y}), ..., \\phi_{K}(\\bf{x}, \\bf{y})) $\n",
    "* $ \\phi_k(\\bf{x}, \\bf{y})=\\sum_t{\\phi_k(\\bf{x}, t, y_t, y_{t-1})} $で$ \\phi_k $を限定(Linear-chain CRF)\n",
    "* $ \\sum_t{(w ・ \\phi_k(\\bf{x}, t, y_t, y_{t-1}))} $の最大値が系列$ \\bf{y} $の推定値となる\n",
    "* $ \\sum_t{\\phi_k(\\bf{x}, t, y_t, y_{t-1})} $：$ x_t $とそのラベル$ y_t $の関係が成立していたら1、成立していなければ0を返す関数　素性関数\n",
    "\n",
    "|  表記  |  品詞  |  ラベル  |\n",
    "| ---- | ---- | ---- |\n",
    "|  田中  |  名詞  |  B-Person  |\n",
    "|  さん  |  接尾  |  O  |\n",
    "|  は  |  助詞  |  O  |\n",
    "|  茨城  |  名詞  |  B-Organization  |\n",
    "|  大学  |  名詞  |  I-Organization  |\n",
    "|  の  |  助詞  |  O  |\n",
    "|  学生  |  名詞  |  O  |\n",
    "|  です  |  助動詞  |  O  |\n",
    "|  。  |  句点  |  O  |\n",
    "\n",
    "* $ \\bf{x}_4 $：[表記：茨城, 品詞：名詞], $ y_4 $：B-Organization\n",
    "* $ \\phi_1 $を「表記が\"茨城\"かつラベルがB-Organization」という条件に設定したとき、条件を満たすのは$ \\phi_1(\\bf{x}, 4, y_3, y_2) $(4行目)のみとなるため、$ \\phi_1(\\bf{x}, \\bf{y})=1 $となる\n",
    "* $ \\phi_2 $を「品詞が\"助詞\"かつラベルがO」という条件に設定したとき、条件を満たすのは$ \\phi_2(\\bf{x}, 3, y_3, y_2) $と$ \\phi $(3行目)と$ \\phi_2(\\bf{x}, 6, y_6, y_5) $(6行目)となるため、$ \\phi_2(\\bf{x}, \\bf{y})=2 $となる\n",
    "* このような操作を繰り返して$ \\phi(\\bf{x}, \\bf{y}) $を作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 品詞タガ－の作成\n",
    "pos = ['S', 'E', '名詞', '動詞', '副詞', '助詞']  #ラベルとなる品詞\n",
    "vocab = ['S', 'E', '私', '犬', 'は', 'を', '愛す', '飼う', '去年', 'いつも']  #入力系列の要素となる単語"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $ p(S_j|S_i) $の初期値設定\n",
    "* $ p(助詞|名詞)=p(S_5|S_2) $は3行6列目の要素→0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "t_mat = np.array([\n",
    "    [0.0, 0.0, 0.6, 0.0, 0.4, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.4, 0.6],\n",
    "    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.4, 0.4, 0.2, 0.0],\n",
    "    [0.0, 0.0, 0.3, 0.4, 0.3, 0.0]        \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $ p(w|S_i) $の初期値設定\n",
    "* $ p(犬|名詞)=p(w_3|S_2) $は3行4列目の要素→0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_mat = np.array([\n",
    "    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0]    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: hmmlearn in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (0.2.7)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from hmmlearn) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from hmmlearn) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from hmmlearn) (1.21.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.16->hmmlearn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.16->hmmlearn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm\n",
    "model = hmm.MultinomialHMM(n_components=len(pos),\n",
    "                           transmat_prior=t_mat,\n",
    "                           params='e',\n",
    "                           init_params='t',\n",
    "                           n_iter=20)\n",
    "model.emissionprob_ = e_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"S 私 は 犬 を 愛す E\"\n",
    "s2 = \"S 去年 犬 を 飼う E\"\n",
    "s3 = \"S 犬 は 私 を 愛す E\"\n",
    "s4 = \"S 去年 私 は いつも 犬 を 飼う E\"\n",
    "s5 = \"S いつも いつも 犬 を 愛す E\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, lens = [], []\n",
    "for s in [s1, s2, s3, s4, s5]:\n",
    "        slist = s.split()\n",
    "        X += [ vocab.index(w) for w in  slist]\n",
    "        lens.append(len(slist))\n",
    "X = np.array(X).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting a model with 54 free scalar parameters with only 36 data points will result in a degenerate solution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialHMM(init_params='t', n_components=6, n_iter=20, params='e',\n",
       "               random_state=RandomState(MT19937) at 0x221EF821340,\n",
       "               transmat_prior=array([[0. , 0. , 0.6, 0. , 0.4, 0. ],\n",
       "       [1. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0.4, 0.6],\n",
       "       [0. , 1. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0.4, 0.4, 0.2, 0. ],\n",
       "       [0. , 0. , 0.3, 0.4, 0.3, 0. ]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, lengths=lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = \"S 犬 は いつも 私 を 愛す E\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [ vocab.index(w) for w in ts.split() ]\n",
    "ts = np.array(ts).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', '名詞', '助詞', '副詞', '名詞', '助詞', '動詞', 'E']\n"
     ]
    }
   ],
   "source": [
    "ans = model.predict(ts)\n",
    "print([pos[p] for p in ans ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn_crfsuite in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (0.3.6)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from sklearn_crfsuite) (0.9.8)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn_crfsuite) (1.16.0)\n",
      "Requirement already satisfied: tabulate in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn_crfsuite) (0.8.9)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn_crfsuite) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=2.0->sklearn_crfsuite) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "train0 = ConllCorpusReader('conll2003', \n",
    "                       'train.txt', \n",
    "                       ['words', 'pos', 'ignore', 'chunk'])\n",
    "train_sents = list(train0.iob_sents())\n",
    "test0 = ConllCorpusReader('conll2003', \n",
    "                       'test.txt', \n",
    "                       ['words', 'pos', 'ignore', 'chunk'])\n",
    "test_sents = list(test0.iob_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14987"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3684"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('EU', 'NNP', 'B-ORG'), ('rejects', 'VBZ', 'O'), ('German', 'JJ', 'B-MISC'), ('call', 'NN', 'O'), ('to', 'TO', 'O'), ('boycott', 'VB', 'O'), ('British', 'JJ', 'B-MISC'), ('lamb', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# 単語表記、品詞、固有表現のタグの3組からなるタプルのリスト\n",
    "print(train_sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),  #単語を小文字にする処理\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),  #wordが大文字であるかをブーリアンで返す処理\n",
    "        'word.istitle()': word.istitle(),  #wordが先頭だけ大文字であるかをブーリアンで返す処理\n",
    "        'word.isdigit()': word.isdigit(),  #wordが数字であるかをブーリアンで返す処理\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14987"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[1])  #EU rejects German call to boycott British lamb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias': 1.0,\n",
       " 'word.lower()': 'german',\n",
       " 'word[-3:]': 'man',\n",
       " 'word[-2:]': 'an',\n",
       " 'word.isupper()': False,\n",
       " 'word.istitle()': True,\n",
       " 'word.isdigit()': False,\n",
       " 'postag': 'JJ',\n",
       " 'postag[:2]': 'JJ',\n",
       " '-1:word.lower()': 'rejects',\n",
       " '-1:word.istitle()': False,\n",
       " '-1:word.isupper()': False,\n",
       " '-1:postag': 'VBZ',\n",
       " '-1:postag[:2]': 'VB',\n",
       " '+1:word.lower()': 'call',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+1:postag': 'NN',\n",
       " '+1:postag[:2]': 'NN'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1][2]  #X_train[1]の2番目の単語\"German\"に関する情報、CRFの素性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,  #L1正則化パラメータ\n",
    "    c2=0.1,  #L2正則化パラメータ\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25680"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crf.state_features_)  #重みベクトルの情報、素性の種類数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.712476"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.state_features_[('word.lower():german', 'B-MISC')]  #素性word.lower():germanとB-MISCの対に対する重み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9564337245612146"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.score(X_test, y_test)  #テストデータに対して固有表現抽出を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_test)  #ラベル'O'を含めずにモデルを評価するためにpredictを用いる、最終的なラベルの系列を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "\n",
    "f1 = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)  #F値の算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8014153821544218"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred = crf.predict_marginals(X_test)  #単語の各ラベルに対する確率を算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-ORG': 0.00045332666293954467,\n",
       " 'O': 0.9984552751743301,\n",
       " 'B-MISC': 6.724150177606486e-05,\n",
       " 'B-PER': 0.000219912398209062,\n",
       " 'I-PER': 2.3138879491281777e-08,\n",
       " 'B-LOC': 0.0007964142557493604,\n",
       " 'I-ORG': 4.3541281190583085e-06,\n",
       " 'I-MISC': 2.40505222223752e-06,\n",
       " 'I-LOC': 1.0476877758934262e-06}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_pred[1][0]  #テストデータの1番目の文の最初の単語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))\n",
    "# print(metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(2.6907), [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1])\n",
      "(tensor(20.4906), [0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n",
    "\n",
    "\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "# Make up some training data\n",
    "training_data = [(\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ")]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "\n",
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "# Check predictions before training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "    print(model(precheck_sent))\n",
    "\n",
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "for epoch in range(\n",
    "        300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is,\n",
    "        # turn them into Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    print(model(precheck_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (略)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
