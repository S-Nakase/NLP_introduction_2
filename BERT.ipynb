{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前学習済みモデルとは？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CNN：オブジェクトに対して特徴ベクトルを作成している\n",
    "* 画像分類→CNNの部分は事前学習済みのものを利用し、NNに部分でモデルを構築すればよい\n",
    "* ここでいうCNNを「事前学習済みモデル」と呼ぶ\n",
    "* 事前学習済みモデル：入力オブジェクトを特徴ベクトルのようなものに変換するモデルであり、その分野の様々なタスクに共通して利用できるネットワークモデル\n",
    "     * 自然言語処理の場合、入力オブジェクトは単語列、タスクへの入力データは対応する単語の埋め込み表現の列となる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 事前学習済みモデルの特徴\n",
    "* 様々なタスクに共通して使えるため、パワフルなものを一つ作っておけばよい→構築は大変だが、作ってしまえば便利\n",
    "* 下流のタスクで必要となるラベル付きデータの量を軽減できる→転移学習、教師ありデータを作成するコストが減る\n",
    "* 下流タスクによって調整可能→fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "* 出力する単語の埋め込み表現は文脈依存になっている、周辺の単語との関係から埋め込み表現を作成する\n",
    "     * 「私は犬が好き。」の「犬/dog」と「奴は警察の犬だ。」の「犬/spy」は語義が異なる\n",
    "* fine-tuningが可能\n",
    "     * HuggingFaceのtransformersというライブラリを用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers[ja] in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (4.21.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from transformers[ja]) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (4.64.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (2022.3.15)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (2.27.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from transformers[ja]) (0.9.1)\n",
      "Requirement already satisfied: ipadic<2.0,>=1.0.0 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from transformers[ja]) (1.0.0)\n",
      "Requirement already satisfied: unidic-lite>=1.0.7 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from transformers[ja]) (1.0.8)\n",
      "Requirement already satisfied: fugashi>=1.0 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from transformers[ja]) (1.2.0)\n",
      "Requirement already satisfied: unidic>=1.0.2 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from transformers[ja]) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[ja]) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers[ja]) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[ja]) (0.4.4)\n",
      "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from unidic>=1.0.2->transformers[ja]) (0.10.1)\n",
      "Requirement already satisfied: plac<2.0.0,>=1.1.3 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from unidic>=1.0.2->transformers[ja]) (1.3.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[ja]) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[ja]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[ja]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[ja]) (2021.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[ja]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(32768, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert  #構造確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchinfo in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (1.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "BertModel                                          --\n",
       "├─BertEmbeddings: 1-1                              --\n",
       "│    └─Embedding: 2-1                              25,165,824\n",
       "│    └─Embedding: 2-2                              393,216\n",
       "│    └─Embedding: 2-3                              1,536\n",
       "│    └─LayerNorm: 2-4                              1,536\n",
       "│    └─Dropout: 2-5                                --\n",
       "├─BertEncoder: 1-2                                 --\n",
       "│    └─ModuleList: 2-6                             --\n",
       "│    │    └─BertLayer: 3-1                         7,087,872\n",
       "│    │    └─BertLayer: 3-2                         7,087,872\n",
       "│    │    └─BertLayer: 3-3                         7,087,872\n",
       "│    │    └─BertLayer: 3-4                         7,087,872\n",
       "│    │    └─BertLayer: 3-5                         7,087,872\n",
       "│    │    └─BertLayer: 3-6                         7,087,872\n",
       "│    │    └─BertLayer: 3-7                         7,087,872\n",
       "│    │    └─BertLayer: 3-8                         7,087,872\n",
       "│    │    └─BertLayer: 3-9                         7,087,872\n",
       "│    │    └─BertLayer: 3-10                        7,087,872\n",
       "│    │    └─BertLayer: 3-11                        7,087,872\n",
       "│    │    └─BertLayer: 3-12                        7,087,872\n",
       "├─BertPooler: 1-3                                  --\n",
       "│    └─Linear: 2-7                                 590,592\n",
       "│    └─Tanh: 2-8                                   --\n",
       "===========================================================================\n",
       "Total params: 111,207,168\n",
       "Trainable params: 111,207,168\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary  #パラメータの様子確認\n",
    "summary(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer  #入力列をid型二変換するためtokenizerを作成\n",
    "tknz = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['私', 'は', '犬', 'が', '好き', '。']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknz.tokenize(\"私は犬が好き。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3946, 897, 3549, 862, 12215, 829, 3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknz.encode(\"私は犬が好き。\")  #各単語を単語idに変換、文頭の特殊トークン[CLS](2)と文末の特殊トークン[SEP](3)が含まれている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3946, 897, 3549, 862, 12215, 829]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknz.encode(\"私は犬が好き。\", add_special_tokens=False)  #特殊トークン非表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  3946,   897,  3549,   862, 12215,   829,     3]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = tknz.encode(\"私は犬が好き。\")\n",
    "x = torch.LongTensor(x).unsqueeze(0)  #Tensorに変換してBERTに入力、unsqueeze(0)で要素が一つのバッチにする\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6633e-01, -7.8501e-02,  1.1767e-03,  ...,  3.6960e-01,\n",
       "           1.6008e-01, -5.5611e-01],\n",
       "         [ 2.7389e-01, -2.8402e-01, -8.4956e-01,  ..., -6.4883e-01,\n",
       "           3.8284e-01, -1.5853e-01],\n",
       "         [-5.7850e-01,  3.8757e-01, -9.7429e-01,  ...,  1.2454e+00,\n",
       "          -4.9265e-01, -3.7446e-01],\n",
       "         ...,\n",
       "         [ 7.1451e-01,  2.8899e-01, -5.4993e-01,  ...,  1.0793e-01,\n",
       "          -1.8923e+00, -8.3096e-01],\n",
       "         [ 3.0506e-01, -7.4390e-01, -6.6757e-01,  ...,  2.6773e-01,\n",
       "          -9.7722e-01, -8.8383e-01],\n",
       "         [ 1.9436e-01, -8.5321e+00, -1.4069e-01,  ...,  7.1000e-03,\n",
       "          -9.3125e-02, -5.4594e-01]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = bert(x)  #BERTの出力は変数yに入っている\n",
    "y.last_hidden_state  #BERTの出力である埋め込み表現の列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.last_hidden_state.shape  #[バッチサイズ, 単語列の長さ, 単語の次元数]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.5998e-01,  1.2333e-01, -4.1867e-01,  1.2998e-01,  1.5049e-01,\n",
       "        -1.1773e+00, -8.8095e-01,  3.9046e-01,  1.1116e-02,  5.0804e-02,\n",
       "         6.2080e-01, -1.4059e+00, -1.5116e+00, -1.0155e+00, -5.3455e-02,\n",
       "         3.3133e-01, -1.2388e+00, -1.4632e+00,  3.0444e+00, -1.0437e+00,\n",
       "        -4.2564e-01,  2.7795e-01, -9.6374e-01, -6.7335e-01, -7.0273e-01,\n",
       "        -2.0534e-01, -3.4195e-02,  1.0376e+00,  3.1639e-01, -9.4961e-01,\n",
       "        -4.9169e-02,  3.3885e-01,  1.7551e+00,  3.5499e-01,  1.4980e+00,\n",
       "         7.2804e-02,  4.8653e-01, -2.7875e-01, -4.2737e-01,  3.1235e-01,\n",
       "         7.8298e-01,  2.4274e-01, -7.7656e-01, -1.8049e-01,  1.1966e+00,\n",
       "        -5.4871e-02,  1.1559e+00,  9.0174e-01, -5.0762e-01, -9.7665e-01,\n",
       "         2.4764e-01, -1.2593e-01, -4.3765e+00, -5.8042e-01,  6.3760e-01,\n",
       "        -1.8688e+00,  2.2698e-01, -2.3121e-01,  4.7407e-01, -2.4936e-01,\n",
       "         1.9515e-01, -8.7563e-01, -1.8446e-01, -7.8302e-01,  1.0440e+00,\n",
       "        -5.5363e-01, -2.9012e-01, -1.5317e+00,  3.8584e-01, -8.6836e-01,\n",
       "        -2.0905e-01, -9.1756e-02,  1.4765e+00, -5.1707e-01,  9.6041e-01,\n",
       "         1.4896e-01,  5.3854e-01, -2.7401e-01,  5.6505e-02, -2.0169e-01,\n",
       "        -6.6927e-01, -2.5052e-01, -1.1689e+00, -1.5851e-02, -4.4305e-01,\n",
       "        -1.4663e+00, -3.2679e-01, -3.1647e-01,  4.1643e-03,  2.7501e-01,\n",
       "        -1.5405e+00,  1.9338e-01,  1.3297e+00,  1.7681e-01, -9.1270e-01,\n",
       "        -4.7032e-01,  4.8026e-01, -6.5800e-01, -1.1496e+00, -5.4585e-01,\n",
       "        -6.4135e-01,  1.4641e-01, -3.3034e-01, -7.4462e-01, -1.6620e+00,\n",
       "         5.9938e-01, -7.4100e-02, -8.0877e-01, -2.0327e+00,  4.7980e-01,\n",
       "         7.6624e-01, -9.1393e-01,  2.6598e-01, -1.7256e+00, -1.3173e+00,\n",
       "        -3.7948e-01,  1.1317e+00,  2.7596e-01, -4.2324e-01,  7.5909e-01,\n",
       "         7.6096e-01,  6.0343e-01,  2.4706e-03, -4.0539e-01, -9.2445e-01,\n",
       "         6.4261e-01, -1.1433e+00, -4.3264e-01,  1.2390e+00, -1.1444e+00,\n",
       "        -9.7230e-02,  2.3155e-01,  7.0384e-02, -6.5266e-01,  1.7787e+00,\n",
       "        -2.0272e+00, -7.8996e-01, -8.2464e-01,  6.3004e-01,  5.5565e-01,\n",
       "        -2.9930e-01, -7.2575e-01, -5.2703e-01, -3.8133e-01,  6.3388e-01,\n",
       "        -5.5153e-01, -9.8513e-01,  6.0027e-01, -4.1644e-01, -5.3161e-01,\n",
       "        -4.1829e-01, -6.3536e-01,  3.3887e-01,  1.1577e+00, -3.2157e-02,\n",
       "         2.7131e-01,  2.6676e-01, -2.3230e-01, -7.9728e-01, -1.3730e+00,\n",
       "        -1.1874e+00, -2.7421e-01,  2.4357e-01,  9.2686e-01, -3.2455e-01,\n",
       "         4.6432e-02, -1.2669e+00, -7.6265e-01, -3.3252e-01,  6.4351e-01,\n",
       "         1.0219e+00,  5.5042e-01, -6.5461e-01,  6.8335e-02,  2.6219e-01,\n",
       "         1.3874e+00,  1.3585e+00, -8.1907e-01, -2.4835e-01,  4.3382e-01,\n",
       "         6.1233e-01, -1.2292e+00,  3.1932e-01,  1.3300e+00,  3.2329e-01,\n",
       "        -7.7025e-01,  4.6598e-01,  1.6611e-01,  4.6045e-01, -9.5581e-01,\n",
       "        -1.3002e+00, -1.7976e-01, -4.4811e-01, -2.8271e-01, -7.9888e-01,\n",
       "         1.1062e+00,  4.6667e-01,  2.5040e+00, -5.0132e-01,  5.8942e-01,\n",
       "         2.4908e-01, -4.9075e-01, -8.6088e-01, -5.4032e-02,  1.2750e+00,\n",
       "         3.7331e-01, -1.1620e+00,  4.7155e-03, -2.9604e-01, -3.4236e-01,\n",
       "        -4.3695e-01,  1.5472e-01, -5.3004e-02,  1.4021e+00,  4.6574e-01,\n",
       "        -1.9112e-01,  6.2127e-01, -5.9934e-01, -1.4941e-01, -5.3493e-01,\n",
       "         2.4627e-02,  6.3412e-01,  1.6706e+00,  4.3146e-02,  6.6929e-01,\n",
       "        -6.7244e-01,  6.9219e-01,  2.9934e-02,  2.9347e-01,  3.6864e-01,\n",
       "         8.9426e-01,  5.6283e-02, -3.5741e-01,  8.9381e-02, -1.4136e+00,\n",
       "        -4.9181e-01,  3.2873e-01, -5.2390e-01, -3.3620e-01,  4.4952e-01,\n",
       "        -1.8059e-01, -5.8400e-01,  1.0763e+00, -9.7750e-02,  7.3988e-01,\n",
       "         3.3248e-01,  4.3454e-02,  1.7525e-01,  2.2037e-01, -1.4335e-01,\n",
       "        -4.8352e-01,  6.2528e-01,  4.7547e-03, -1.6418e-01, -6.8599e-01,\n",
       "        -1.2886e-02,  2.3579e-01,  5.1027e-01, -8.4324e-02,  1.9178e-01,\n",
       "         2.1582e-01,  2.1841e+00, -2.0570e-01,  1.4139e-01,  2.4457e+00,\n",
       "        -4.4328e-01,  6.2814e-01,  4.0695e-01,  5.1243e-01, -3.8122e-01,\n",
       "         5.5976e-01, -7.7957e-02, -5.0273e-01,  5.9018e-02,  5.2933e-01,\n",
       "         5.3657e-01,  1.6380e-01, -7.6380e-02, -5.3196e-02,  4.7217e-02,\n",
       "        -1.4752e+00,  8.6445e-01, -2.1844e-01, -3.4345e-01, -3.1443e-01,\n",
       "        -2.3642e-01,  3.1615e-02, -2.4453e-02,  9.1187e-01, -1.9337e-01,\n",
       "         5.8253e-01, -3.1557e-01,  2.6214e-01,  1.7525e+00, -4.7159e-01,\n",
       "        -5.9733e-01, -8.9219e-01, -9.2374e-02,  7.3485e-01,  3.2334e-01,\n",
       "        -1.6766e+00, -4.5738e-01, -2.1092e-01, -7.6721e-01,  2.4080e-02,\n",
       "        -3.2697e-01, -1.3408e-01,  1.2251e-02, -1.7073e+00,  9.1963e-01,\n",
       "         6.1411e-01, -1.5626e-01, -1.6888e-01,  8.4162e-01, -9.5892e-01,\n",
       "        -1.0844e+00,  1.4881e-01,  1.9632e-01,  7.7409e-02,  1.4156e-01,\n",
       "        -2.2834e-01, -1.1843e-01, -1.2503e+00,  7.4304e-01,  1.4416e+00,\n",
       "        -3.3950e-01,  5.6264e-01,  1.9289e-03, -5.8882e-01, -6.8578e-01,\n",
       "         1.2650e-02,  2.2117e-01,  1.0095e-01,  1.9565e-02,  6.6394e-01,\n",
       "        -6.3752e-01, -1.6711e-01, -5.9285e-01, -4.7668e-01,  8.8474e-01,\n",
       "         6.1301e-01, -1.7508e-01, -7.1140e-01,  2.4005e-01,  1.1462e-01,\n",
       "        -7.4380e-01,  1.4700e+00, -1.9925e-01, -2.1531e-01, -1.2251e+00,\n",
       "         3.0770e-01,  6.8797e-01, -5.6631e-01,  8.2678e-01,  3.4808e-01,\n",
       "        -5.6226e-01, -9.9413e-01,  1.1029e+00, -1.1537e-01, -4.8309e-01,\n",
       "        -7.9505e-01, -8.9807e-02, -5.4381e-01,  1.0504e+00,  8.9512e-01,\n",
       "         4.9797e-01,  4.9560e-01,  1.1698e+00, -1.4465e-01,  6.1242e-01,\n",
       "        -2.3821e-01, -1.1307e+00, -6.5506e-01,  1.1397e+00,  4.8889e-01,\n",
       "        -1.2073e+00,  9.7565e-01, -3.8665e-01,  1.1495e+00,  1.9920e-01,\n",
       "         6.0732e-01,  7.3402e-01, -3.2823e-01,  6.6685e-02,  5.9845e-01,\n",
       "         1.1011e+00, -1.0330e+00,  4.2111e-01,  1.1999e-01,  4.6728e-02,\n",
       "         7.7463e-01, -6.1593e-01, -1.2715e+00,  4.0782e-01, -3.8996e-01,\n",
       "         7.0634e-02,  3.0282e-01, -1.6991e-01, -4.4998e-02,  1.1125e+00,\n",
       "        -1.1480e+00, -1.0587e+00, -4.2897e-01, -7.5486e-01, -1.8897e-01,\n",
       "         1.2163e+00, -7.0276e-01,  3.7787e-01, -2.8954e-01, -2.2572e-01,\n",
       "         1.4250e+00, -8.4101e-01,  4.6942e-01, -9.2195e-01, -6.1209e-01,\n",
       "         8.0839e-01, -5.8000e-01,  2.1487e-01,  3.3778e+00, -3.0348e-01,\n",
       "         1.4282e+00, -1.1809e+00, -1.2210e-01,  1.6974e-01, -2.3317e-01,\n",
       "        -1.0645e+00, -4.6249e-01, -3.8726e-01,  6.1198e-01, -1.7527e-01,\n",
       "         1.0208e+00,  1.0293e+00,  7.8348e-01,  4.5547e-02,  1.3896e+00,\n",
       "         2.4822e-01,  3.6393e-01,  1.4609e-01,  2.9588e-01, -1.6030e+00,\n",
       "        -1.7084e-01, -5.9474e-01,  1.2856e+00, -4.3874e-01,  2.1327e-01,\n",
       "        -6.1715e-01,  1.1980e+00, -6.1747e-01,  1.6338e-01,  1.3043e+00,\n",
       "         1.3634e-02,  8.0502e-01, -1.6922e-01, -6.4294e-01,  4.9836e-01,\n",
       "         2.6138e-01,  4.3368e-02,  2.5224e-01, -2.7753e-01,  4.3361e-01,\n",
       "        -8.1289e-01,  3.0330e-01, -4.4576e-01, -4.3434e-01, -8.3705e-01,\n",
       "        -3.1789e-01,  6.6209e-01, -9.6268e-01, -1.4616e+00,  6.2674e-01,\n",
       "        -5.8100e-01, -8.1699e-02, -1.7603e-01,  3.7135e-01, -4.4851e-01,\n",
       "        -1.5155e-01, -8.8182e-01,  1.9230e+00,  6.6994e-01, -8.8451e-01,\n",
       "        -9.7270e-01,  4.5804e-01,  1.9571e-01,  1.1892e+00,  1.1664e-01,\n",
       "         6.6026e-01,  1.9542e-01, -5.0468e-01,  3.4971e-01, -7.5621e-01,\n",
       "         7.9065e-01, -3.1511e-01, -1.7366e+00, -3.2171e-01, -7.0650e-01,\n",
       "        -1.0271e+00,  2.7688e-01, -6.5178e-01, -6.2081e-01,  7.5161e-01,\n",
       "        -2.1265e-01,  6.7958e-01, -2.4216e-01,  1.5087e-01,  2.8519e-01,\n",
       "        -6.0830e-01, -6.6821e-01, -1.7994e-01,  7.5535e-01, -1.2232e-01,\n",
       "        -1.6204e-01, -5.1012e-01, -3.3225e-01, -1.1444e+00,  9.6182e-01,\n",
       "         8.5121e-02,  1.0562e+00,  1.5191e+00, -2.2429e-01, -2.7118e-03,\n",
       "         3.6326e-01, -1.4854e-01, -6.3623e-02, -1.8995e-01,  1.0904e+00,\n",
       "         4.8412e-01, -2.3793e-01, -1.0808e-01,  2.8676e-01, -1.6041e-01,\n",
       "         8.9801e-01,  5.8945e-01,  7.3459e-01,  9.6137e-01,  1.6483e+00,\n",
       "         1.1829e-01,  1.5476e+00, -4.7707e-01, -4.3979e-01,  5.9378e-02,\n",
       "         4.7717e-01, -2.9899e-01,  5.3822e-01, -6.5502e-01,  5.2132e-01,\n",
       "         5.9853e-02,  6.4549e-01,  1.1591e+00, -1.8834e-01,  5.0871e-01,\n",
       "        -8.7178e-01,  3.9662e-01, -3.6634e-02, -2.7102e-01, -1.0376e+00,\n",
       "        -3.9104e-01,  9.2132e-01, -7.0803e-01, -9.6286e-01,  1.0882e+00,\n",
       "         3.0118e-01, -8.1267e-01,  3.8535e-01, -6.0028e-01,  3.0585e-01,\n",
       "         1.2196e+00, -1.0701e+00,  3.9756e-01,  9.4746e-01,  1.8190e+00,\n",
       "         3.6438e-01,  8.6656e-01, -1.2668e+00,  6.4570e-01,  6.4274e-01,\n",
       "         1.7152e+00, -8.8871e-01, -6.4040e-02,  7.8097e-01, -6.8617e-01,\n",
       "         1.5846e-01,  4.1687e-01,  4.4013e-01,  9.2187e-01,  2.4501e-01,\n",
       "         1.1721e+00,  1.0892e+00,  3.8711e-01,  6.1568e-01, -6.4919e-01,\n",
       "        -1.7959e-01, -2.4205e-01,  2.5268e-01,  6.5086e-01, -1.5588e-01,\n",
       "         4.9747e-02,  1.8400e+00,  1.1344e-01,  4.0495e-02, -1.8074e-01,\n",
       "         2.1394e+00,  7.3046e-01,  2.6881e-01,  2.0732e-02,  4.6137e-01,\n",
       "        -1.0741e+00,  7.9161e-01, -4.4372e-01,  1.0573e+00, -5.6358e-01,\n",
       "        -9.6237e-01,  5.8992e-01,  5.5276e-01, -5.5178e-01, -2.2338e-01,\n",
       "        -1.1385e+00, -1.1462e-01,  4.8217e-01, -2.7742e-02,  2.3937e-01,\n",
       "         1.8090e-01, -5.9373e-01,  9.1835e-02, -1.1463e+00,  1.7783e-01,\n",
       "         7.9076e-01,  1.3599e+00,  9.4021e-02, -6.2239e-01, -3.0779e-01,\n",
       "         2.0459e-01, -2.3920e-01,  1.2064e+00,  3.5463e-01,  2.7906e-01,\n",
       "        -4.5379e-01, -8.4819e-02, -6.0274e-01,  6.8782e-01,  2.7790e-02,\n",
       "         1.3533e+00,  1.8558e-01, -5.0861e-01,  4.0797e-01, -1.4091e+00,\n",
       "         1.0439e+00,  7.4485e-01, -6.5988e-02, -8.9791e-01, -6.6319e-01,\n",
       "         4.3678e-01, -3.1024e-01,  4.3888e-01,  1.6126e-01,  2.1729e-01,\n",
       "         6.2809e-01,  9.1303e-01,  1.2299e+00, -7.2054e-01,  3.5526e-01,\n",
       "        -1.1807e-01, -6.0173e-02, -6.2963e-01,  2.7142e-01, -6.5090e-01,\n",
       "         8.1006e-01, -1.2393e-01,  1.4478e+00,  3.2399e-01,  4.8201e-01,\n",
       "         1.1299e-01, -8.1906e-01, -9.7838e-01, -9.9647e-03,  3.6551e-01,\n",
       "        -2.4772e-01,  6.2413e-01,  5.6947e-02,  2.6786e-01,  3.3552e-01,\n",
       "         1.1783e-01,  3.6284e-03, -8.1315e-01,  2.0859e-01, -2.5847e-01,\n",
       "         4.9114e-01,  1.0965e+00, -2.8053e-01,  1.8367e-01, -2.4723e+00,\n",
       "         3.2128e-01,  7.7412e-01,  1.8395e-01,  6.7091e-01, -8.2781e-01,\n",
       "         3.2847e-01, -7.2773e-01,  8.3497e-01,  6.3675e-01, -1.6732e-01,\n",
       "        -4.9941e-01, -6.4598e-03,  1.3821e+00, -4.0015e-01,  8.6646e-02,\n",
       "         2.0658e-01,  4.6257e-01, -4.6218e-01, -1.0999e+00, -1.0173e+00,\n",
       "        -1.6799e-01, -1.2457e+00, -1.2064e-01,  2.7568e-01,  1.7410e+00,\n",
       "         1.4829e+00,  2.3641e-01, -1.7259e-02,  8.5032e-01,  3.2740e-01,\n",
       "         5.0344e-01, -7.6481e-02,  2.5152e-01,  6.1683e-01, -1.4734e-01,\n",
       "         6.9943e-01,  8.1998e-01, -5.8856e-01, -1.5573e+00, -3.4539e-01,\n",
       "        -2.1353e-03, -9.8463e-01,  4.6256e-03, -2.1918e-01,  2.2653e-01,\n",
       "         1.5436e+00, -3.4148e-01, -1.3844e+00,  7.1271e-01, -5.5610e-01,\n",
       "        -2.2625e-01, -4.0399e-01,  5.5933e-01,  1.0119e+00,  1.1700e+00,\n",
       "         1.1880e+00, -4.4626e-01, -1.3521e+00, -2.3402e-01, -3.3009e-01,\n",
       "         3.3759e-01,  4.9390e-01, -1.0302e+00,  7.5773e-01,  2.4563e-02,\n",
       "        -1.8915e+00,  8.8234e-01, -1.0214e+00,  1.1322e+00, -1.6862e+00,\n",
       "        -1.2719e+00, -9.0147e-01, -1.6515e-03, -1.4982e-01,  2.1852e-01,\n",
       "        -1.0359e-01,  8.9650e-01,  9.7352e-02], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.last_hidden_state[0][3]  #入力文内の4番目の単語「犬」の埋め込み表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT内部の処理\n",
    "* Transformer：BERTのアーキテクチャの一種\n",
    "     * ニューラル機械翻訳モデル\n",
    "     * 大きくEncoderとDecoderから構成されており、Encoder部をBERTと呼ぶ\n",
    "     * BERTはBertEmbeddingsとBertEncoderから構成されており、BertEmbeddingsにはPositional Encodingという部分がある\n",
    "     * Positional Encodingで単語を分散表現に変換している\n",
    "     * BertEmbeddingsで作られた単語の埋め込み表現列を12個のBertLayer(BertEncoder)を使って少しずつ修正している\n",
    "     \n",
    "* Position Embeddings：単語の位置を抽象的なオブジェクトと見なして、n次元空間に埋め込むこと\n",
    "     * BertEmbeddingsでは、単語の分散表現ベクトル・Position Embeddings・Segment Embeddingsの３つの埋め込み表現のベクトルの和を作る処理が行われている\n",
    "     * Segment Embeddings：1文目の単語か2文目の単語かを表す埋め込み表現のベクトル\n",
    "\n",
    "* BertLayer：単語の埋め込み表現列が入力される部分\n",
    "     * この入力がMulti-Head Attentionに渡され、単語の埋め込み表現列が出力される\n",
    "     * ここで得られた出力に対して、残差接続を行い、その結果に対してLayer Normalizationの処理を行う\n",
    "     * その結果を線形変換し、もう一度同様の操作を行う\n",
    "\n",
    "* Multi-Head Attention：(略)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTによる文書分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'cl-tohoku/bert-base-japanese-v2',\n",
    "    num_labels = 3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"\"\"\n",
    "私は犬が好きです。一般に動物が好きです。\n",
    "言葉が使える動物がいたら楽しいと思います。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer\n",
    "tknz = BertJapaneseTokenizer.from_pretrained(\n",
    "    'cl-tohoku/bert-base-japanese-v2')\n",
    "x = tknz.encode(d)  #\n",
    "x = torch.LongTensor(x).unsqueeze(0)\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.logits.shape  #[バッチサイズ, ラベル数]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3156, -0.1285,  0.2848], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.logits[0]  #最大のもののラベルを推定結果とすればよい。今回は第2のラベル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "opt = optim.SGD([{'params':model.parameters(), 'lr':0.01}])  #最適化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga = torch.LongTensor([1]).unsqueeze(0)  #文書dの正解ラベルが1であるときの正解ラベルのデータの出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x, labels=ga)  #学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = y.loss  #損失値の出力\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mymodel.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('myconfig.pkl', 'bw') as fw:\n",
    "    pickle.dump(model.config, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('myconfig.pkl', 'br') as f:\n",
    "    myconfig = pickle.load(f)\n",
    "    mymodel = BertForSequenceClassification(config=myconfig)\n",
    "    mymodel.load_state_dict(torch.load('mymodel.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTによる系列ラベリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* クラスBertForTokenClassificationを利用\n",
    "* 入力となるデータ系列の各データにラベルを付与するタスクに適している\n",
    "* 各tokenに対する各ラベルの確率からビタビアルゴリズムを利用して最終的なラベルを決定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "                 'cl-tohoku/bert-base-japanese-v2',\n",
    "                 num_labels = 9)  #num_labels：ラベル数の指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"田中さんは茨城大学の学生です。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['田中', 'さん', 'は', '茨城', '大学', 'の', '学生', 'です', '。']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer\n",
    "tknz = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v2')\n",
    "tknz.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 13026, 11689, 897, 14121, 11188, 896, 12229, 12461, 829, 3]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tknz.encode(s)  #単語id列に変換\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor(x).unsqueeze(0)\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 9])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.logits.shape  #属性logits、[バッチサイズ, 単語数, ラベル数]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 9])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.logits[0].shape  #各単語に対する各ラベルのlogit値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "opt = optim.SGD([{'params':model.parameters(), 'lr':0.01}])  #最適化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ラベル名とラベルidの対応→テキスト参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga = torch.LongTensor([0, 1, 0, 0, 3, 7, 0, 0, 0, 0, 0]).unsqueeze(0)  #正解のラベル列のデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x, labels=ga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = y.loss\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelineによるタスクの推論\n",
    "* pipeline：各種タスクに対する事前学習済みモデルを利用して、そのタスクの推論処理を行ってくれるコマンド\n",
    "\n",
    "### タスクの種類\n",
    "* conversational：対話\n",
    "* feature-extraction：特徴抽出\n",
    "* fill-mask：マスク指定(隠す単語を指定して、推論を行わせる)\n",
    "* image-classification：画像識別\n",
    "* question-answering：質問応答\n",
    "* table-question-answering：表内容からの質問応答\n",
    "* text2text-generation：翻訳、要約、質問応答\n",
    "* text-classification / sentiment-analysis：評判分析\n",
    "* text-generation：テキスト生成\n",
    "* token-classification / ner：固有表現抽出\n",
    "* translation：翻訳\n",
    "* translation_xx_to_yy：XX_YY翻訳\n",
    "* summarization：要約\n",
    "* zero-shot-classification：Zero-shot分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評判分析\n",
    "* 入力文が肯定的か否定的かを判定するタスク\n",
    "* 文書分類と基本的に同じ\n",
    "* レビュー分析とかに使える？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337a6e7ca3224e7bbdd400374ad3a966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7b9d8940d94d1dbe317e6abd83449b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a813edaae7a4468a883b032b5d35bd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56d455976134f42a75abb9d6535ae6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "net = pipeline('text-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998513460159302}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'This book is very interesting.'\n",
    "net(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996337890625}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = 'This book has some interesting parts.'\n",
    "net(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* roberta-large-mnli：事前学習済みモデルRoBERTa-largeとデータセットMNLIを利用して学習されたモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8931466ec74466a5cfa8de6378d424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b28f3adb93d489eb703045afe506f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a4a7f530e442848e1bbe41a3918493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6884a3ff24ef4496996eea20477e7749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337a6f43eded441eba07a654eb5fc465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net2 = pipeline('text-classification', model='roberta-large-mnli')  #positive, neutral, negativeで判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEUTRAL', 'score': 0.6103535294532776}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* daigo/bert-base-japanese-sentiment：日本語の評判分析モデルの１つ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net3 = pipeline('text-classification',\n",
    "#                model='daigo/bert-base-japanese-sentiment')\n",
    "# net3(\"この犬は本当にお利口さんだ。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 固有表現抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4251b7208174dc4b55245566e840131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1923a1c49bf348c5a43e611ea7ff4020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fbdc91de1c472191da262414792e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ab1e3d68164c959d7f9b65b73e3ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99895644,\n",
       "  'index': 3,\n",
       "  'word': 'Tanaka',\n",
       "  'start': 3,\n",
       "  'end': 9},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9990582,\n",
       "  'index': 8,\n",
       "  'word': 'I',\n",
       "  'start': 26,\n",
       "  'end': 27},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.99033654,\n",
       "  'index': 9,\n",
       "  'word': '##bara',\n",
       "  'start': 27,\n",
       "  'end': 31},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9977101,\n",
       "  'index': 10,\n",
       "  'word': '##ki',\n",
       "  'start': 31,\n",
       "  'end': 33},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9953762,\n",
       "  'index': 11,\n",
       "  'word': 'University',\n",
       "  'start': 34,\n",
       "  'end': 44}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = pipeline('ner')\n",
    "text = 'Mr.Tanaka is a student at Ibaraki University.'\n",
    "net(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dslim/bert-base-NER：BERTとCoNLL-2003を使って学習されたモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net2 = pipeline('ner',model='dslim/bert-base-NER')\n",
    "# net2(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要約\n",
    "* デフォルトの事前学習済みモデル：sshleifer/distilbart-cnn-12-6\n",
    "* BART(テキスト生成に利用される事前学習済みモデル)とデータセットCNN/DailyMailDatasetを利用して学習されたモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56699a36f0ee4a65ae8040ce60d1b8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dcab44b29642d7b34187e62a936593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478dbea351cf491984724529e1873873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028945c15d9249c788df8497d1da80d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' BERT is designed to pre-train deep bidirectional representations from unlabeled text by conditioning on both left and right context in all layers . The pre-trained BERT model can be fine-tuned with just one'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = pipeline('summarization')\n",
    "doc = \"\"\"We introduce a new language representation \n",
    "model called BERT, which stands for Bidirectional Encoder \n",
    "Representations from Transformers. Unlike recent language \n",
    "representation models, BERT is designed to pre-train deep \n",
    "bidirectional representations from unlabeled text by \n",
    "jointly conditioning on both left and right context in all \n",
    "layers. As a result, the pre-trained BERT model can be \n",
    "fine-tuned with just one additional output layer to create \n",
    "state-of-the-art models for a wide range of tasks, such as \n",
    "question answering and language inference, without \n",
    "substantial task-specific architecture modifications. BERT \n",
    "is conceptually simple and empirically powerful. It obtains \n",
    "new state-of-the-art results on eleven natural language \n",
    "processing tasks, including pushing the GLUE score to 80.5% \n",
    "(7.7% point absolute improvement), MultiNLI accuracy to \n",
    "86.7% (4.6% absolute improvement), SQuAD v1.1 question \n",
    "answering Test F1 to 93.2 (1.5 point absolute improvement) \n",
    "and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute \n",
    "improvement).\"\"\"\n",
    "net(doc, max_length=50, min_length=20)  #要約の長さ指定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 質問応答\n",
    "* デフォルトの事前学習済みモデル：distilbert-base-cased-distilled-squad\n",
    "* 事前学習済みモデルDistilBERTとデータセットSQuADを利用して学習されたモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1491d9db0b24c5db21474c7f8b54a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a828ce5825994076973044a83daacd60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1d08d2839a4101a182107fc3e68715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b68ada6a4f435f984e081b8dbab447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b1d1bc5ca34042a0f2bea51e907e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.7516631484031677, 'start': 716, 'end': 722, 'answer': 'eleven'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = pipeline('question-answering')\n",
    "q = \"How many tasks did BERT get SOTA on?\"\n",
    "net(context=doc, question=q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキスト生成\n",
    "* デフォルトの事前学習済みモデル：GPT-2\n",
    "* 途中までの文章を与え、それらしい文を生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146b3c4ecd04409d92f701d5142e10e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d4596b32314c7b80fd7654b29cd45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba1a162b0a4457b89c436157bb8a7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4639560e99ab412ab9b624299393a41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b88ef65b55a442080f4149862c7b705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\snaka\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this paper, we propose a new approach based on the observation that the rate of weight loss in adult Chinese women is almost 3 times that in adult men, in comparison to the rate of weight loss in Western women over the same time period. This'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = pipeline('text-generation')\n",
    "net(\"In this paper, we propose a new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot分類\n",
    "* 省略"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
