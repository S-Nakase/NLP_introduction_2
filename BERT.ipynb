{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前学習済みモデルとは？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たとえば、画像分類の分野では...\n",
    "* 事前学習済みモデルCNN + 学習の対象NN → 画像分類完了\n",
    "* まず、「犬」と「猫」を識別するためのCNNを作成し、オブジェクトを特徴ベクトルに変換する方法を学習する\n",
    "     * たとえば、この辺りに縦線がある、この辺りに丸い物体がある、etc...\n",
    "* CNNの部分は事前学習済みのものを利用し、NNの部分でモデルを構築すればよい\n",
    "* つまり、事前学習済みモデルCNNを作成しておけば、学習の対象NNが変わるたびに事前学習をいちいち行う必要がない→効率的な解析が可能\n",
    "* 事前学習済みモデル：入力オブジェクトを特徴ベクトルのようなものに変換するモデルであり、その分野の様々なタスクに共通して利用できるネットワークモデル\n",
    "     * 自然言語処理の場合、入力オブジェクトは単語列、タスクへの入力データは対応する単語の埋め込み表現(分散表現)の列となる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 事前学習済みモデルの特徴\n",
    "* 様々なタスクに共通して使えるため、パワフルなものを一つ作っておけばよい→構築は大変だが、作ってしまえば便利\n",
    "* 下流のタスクで必要となるラベル付きデータの量を軽減できる(転移学習)→教師ありデータを作成するコストが減る\n",
    "* 下流タスクによって調整可能→fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT(Bidirectional Encoder Representations from Transformers)\n",
    "* 2018年末に出現→最先端！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vecによる分散表現との違い\n",
    "* 出力する単語の埋め込み表現は文脈依存になっている、周辺の単語との関係から埋め込み表現を作成する\n",
    "     * 「私は犬が好き。」の「犬/dog」と「奴は警察の犬だ。」の「犬/spy」は語義が異なる\n",
    "     * Word2Vecでは、どちらの場合でも分散表現は固定\n",
    "* 事前学習済みモデルであるため、fine-tuningが可能\n",
    "     * Word2Vecは辞書のようなものであるため、タスクに応じて修正不可\n",
    "     * BERTでは、タスクに応じてネットワークの重みを修正可能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* transformers [HuggingFace] のインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers[ja] in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (4.21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (1.21.5)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (2022.3.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from transformers[ja]) (0.9.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[ja]) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from transformers[ja]) (0.12.1)\n",
      "Requirement already satisfied: unidic-lite>=1.0.7 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from transformers[ja]) (1.0.8)\n",
      "Requirement already satisfied: ipadic<2.0,>=1.0.0 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from transformers[ja]) (1.0.0)\n",
      "Requirement already satisfied: fugashi>=1.0 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from transformers[ja]) (1.2.0)\n",
      "Requirement already satisfied: unidic>=1.0.2 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from transformers[ja]) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[ja]) (4.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers[ja]) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[ja]) (0.4.4)\n",
      "Requirement already satisfied: plac<2.0.0,>=1.1.3 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from unidic>=1.0.2->transformers[ja]) (1.3.5)\n",
      "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (from unidic>=1.0.2->transformers[ja]) (0.10.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[ja]) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[ja]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[ja]) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[ja]) (3.3)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[ja]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 日本語版BERTであるcl-tohoku/bert-base-japanese-v2を学習させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torchinfoのsummaryを使うとBERT内のパラメータの様子を確認できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torchinfo in c:\\users\\snaka\\appdata\\roaming\\python\\python39\\site-packages (1.7.0)\n"
     ]
    }
   ],
   "source": [
    "pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "BertModel                                          --\n",
       "├─BertEmbeddings: 1-1                              --\n",
       "│    └─Embedding: 2-1                              25,165,824\n",
       "│    └─Embedding: 2-2                              393,216\n",
       "│    └─Embedding: 2-3                              1,536\n",
       "│    └─LayerNorm: 2-4                              1,536\n",
       "│    └─Dropout: 2-5                                --\n",
       "├─BertEncoder: 1-2                                 --\n",
       "│    └─ModuleList: 2-6                             --\n",
       "│    │    └─BertLayer: 3-1                         7,087,872\n",
       "│    │    └─BertLayer: 3-2                         7,087,872\n",
       "│    │    └─BertLayer: 3-3                         7,087,872\n",
       "│    │    └─BertLayer: 3-4                         7,087,872\n",
       "│    │    └─BertLayer: 3-5                         7,087,872\n",
       "│    │    └─BertLayer: 3-6                         7,087,872\n",
       "│    │    └─BertLayer: 3-7                         7,087,872\n",
       "│    │    └─BertLayer: 3-8                         7,087,872\n",
       "│    │    └─BertLayer: 3-9                         7,087,872\n",
       "│    │    └─BertLayer: 3-10                        7,087,872\n",
       "│    │    └─BertLayer: 3-11                        7,087,872\n",
       "│    │    └─BertLayer: 3-12                        7,087,872\n",
       "├─BertPooler: 1-3                                  --\n",
       "│    └─Linear: 2-7                                 590,592\n",
       "│    └─Tanh: 2-8                                   --\n",
       "===========================================================================\n",
       "Total params: 111,207,168\n",
       "Trainable params: 111,207,168\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 入力単語列を単語id型に変換するためにtokenizerを作成\n",
    "* 日本語BERTに対応したtokenizerを用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "tknz = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 単語分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['私', 'は', '犬', 'が', '好き', '。']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknz.tokenize(\"私は犬が好き。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 各単語を単語idに変換\n",
    "* 「私」が3946、...、「。」が829に対応\n",
    "* 文頭の特殊トークンである [CLS] (2)と文末の特殊トークン [SEP] (3)が加わっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3946, 897, 3549, 862, 12215, 829, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknz.encode(\"私は犬が好き。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 特殊トークン非表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3946, 897, 3549, 862, 12215, 829]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknz.encode(\"私は犬が好き。\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensor型に変換してBERTに入力、unsqueeze(0)で要素が一つのバッチにする\n",
    "     * 最適なパラメータを求める際に、勾配降下法を用いる\n",
    "     * 勾配降下法を用いるには、学習するデータセットをいくつかのグループに分ける必要がある\n",
    "     * このグループの数を「バッチ」と呼ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  3946,   897,  3549,   862, 12215,   829,     3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = tknz.encode(\"私は犬が好き。\")\n",
    "x = torch.LongTensor(x).unsqueeze(0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BERTの出力結果は変数yに入っている\n",
    "* last_hidden_stateで、BERTの出力である分散表現の列を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6633e-01, -7.8501e-02,  1.1767e-03,  ...,  3.6960e-01,\n",
       "           1.6008e-01, -5.5611e-01],\n",
       "         [ 2.7389e-01, -2.8402e-01, -8.4956e-01,  ..., -6.4883e-01,\n",
       "           3.8284e-01, -1.5853e-01],\n",
       "         [-5.7850e-01,  3.8757e-01, -9.7429e-01,  ...,  1.2454e+00,\n",
       "          -4.9265e-01, -3.7446e-01],\n",
       "         ...,\n",
       "         [ 7.1451e-01,  2.8899e-01, -5.4993e-01,  ...,  1.0793e-01,\n",
       "          -1.8923e+00, -8.3096e-01],\n",
       "         [ 3.0506e-01, -7.4390e-01, -6.6757e-01,  ...,  2.6773e-01,\n",
       "          -9.7722e-01, -8.8383e-01],\n",
       "         [ 1.9436e-01, -8.5321e+00, -1.4069e-01,  ...,  7.1000e-03,\n",
       "          -9.3125e-02, -5.4594e-01]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = bert(x)\n",
    "y.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* last_hidden_stateの形状は[バッチサイズ, 単語列の長さ, 単語の次元数]で表される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [CLS] 「私」「は」「犬」「が」「好き」「。」[SEP] の4番目(0から始まる)の埋め込み表現(犬)を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.5998e-01,  1.2333e-01, -4.1867e-01,  1.2998e-01,  1.5049e-01,\n",
       "        -1.1773e+00, -8.8095e-01,  3.9046e-01,  1.1116e-02,  5.0804e-02,\n",
       "         6.2080e-01, -1.4059e+00, -1.5116e+00, -1.0155e+00, -5.3455e-02,\n",
       "         3.3133e-01, -1.2388e+00, -1.4632e+00,  3.0444e+00, -1.0437e+00,\n",
       "        -4.2564e-01,  2.7795e-01, -9.6374e-01, -6.7335e-01, -7.0273e-01,\n",
       "        -2.0534e-01, -3.4195e-02,  1.0376e+00,  3.1639e-01, -9.4961e-01,\n",
       "        -4.9169e-02,  3.3885e-01,  1.7551e+00,  3.5499e-01,  1.4980e+00,\n",
       "         7.2804e-02,  4.8653e-01, -2.7875e-01, -4.2737e-01,  3.1235e-01,\n",
       "         7.8298e-01,  2.4274e-01, -7.7656e-01, -1.8049e-01,  1.1966e+00,\n",
       "        -5.4871e-02,  1.1559e+00,  9.0174e-01, -5.0762e-01, -9.7665e-01,\n",
       "         2.4764e-01, -1.2593e-01, -4.3765e+00, -5.8042e-01,  6.3760e-01,\n",
       "        -1.8688e+00,  2.2698e-01, -2.3121e-01,  4.7407e-01, -2.4936e-01,\n",
       "         1.9515e-01, -8.7563e-01, -1.8446e-01, -7.8302e-01,  1.0440e+00,\n",
       "        -5.5363e-01, -2.9012e-01, -1.5317e+00,  3.8584e-01, -8.6836e-01,\n",
       "        -2.0905e-01, -9.1756e-02,  1.4765e+00, -5.1707e-01,  9.6041e-01,\n",
       "         1.4896e-01,  5.3854e-01, -2.7401e-01,  5.6505e-02, -2.0169e-01,\n",
       "        -6.6927e-01, -2.5052e-01, -1.1689e+00, -1.5851e-02, -4.4305e-01,\n",
       "        -1.4663e+00, -3.2679e-01, -3.1647e-01,  4.1643e-03,  2.7501e-01,\n",
       "        -1.5405e+00,  1.9338e-01,  1.3297e+00,  1.7681e-01, -9.1270e-01,\n",
       "        -4.7032e-01,  4.8026e-01, -6.5800e-01, -1.1496e+00, -5.4585e-01,\n",
       "        -6.4135e-01,  1.4641e-01, -3.3034e-01, -7.4462e-01, -1.6620e+00,\n",
       "         5.9938e-01, -7.4100e-02, -8.0877e-01, -2.0327e+00,  4.7980e-01,\n",
       "         7.6624e-01, -9.1393e-01,  2.6598e-01, -1.7256e+00, -1.3173e+00,\n",
       "        -3.7948e-01,  1.1317e+00,  2.7596e-01, -4.2324e-01,  7.5909e-01,\n",
       "         7.6096e-01,  6.0343e-01,  2.4706e-03, -4.0539e-01, -9.2445e-01,\n",
       "         6.4261e-01, -1.1433e+00, -4.3264e-01,  1.2390e+00, -1.1444e+00,\n",
       "        -9.7230e-02,  2.3155e-01,  7.0384e-02, -6.5266e-01,  1.7787e+00,\n",
       "        -2.0272e+00, -7.8996e-01, -8.2464e-01,  6.3004e-01,  5.5565e-01,\n",
       "        -2.9930e-01, -7.2575e-01, -5.2703e-01, -3.8133e-01,  6.3388e-01,\n",
       "        -5.5153e-01, -9.8513e-01,  6.0027e-01, -4.1644e-01, -5.3161e-01,\n",
       "        -4.1829e-01, -6.3536e-01,  3.3887e-01,  1.1577e+00, -3.2157e-02,\n",
       "         2.7131e-01,  2.6676e-01, -2.3230e-01, -7.9728e-01, -1.3730e+00,\n",
       "        -1.1874e+00, -2.7421e-01,  2.4357e-01,  9.2686e-01, -3.2455e-01,\n",
       "         4.6432e-02, -1.2669e+00, -7.6265e-01, -3.3252e-01,  6.4351e-01,\n",
       "         1.0219e+00,  5.5042e-01, -6.5461e-01,  6.8335e-02,  2.6219e-01,\n",
       "         1.3874e+00,  1.3585e+00, -8.1907e-01, -2.4835e-01,  4.3382e-01,\n",
       "         6.1233e-01, -1.2292e+00,  3.1932e-01,  1.3300e+00,  3.2329e-01,\n",
       "        -7.7025e-01,  4.6598e-01,  1.6611e-01,  4.6045e-01, -9.5581e-01,\n",
       "        -1.3002e+00, -1.7976e-01, -4.4811e-01, -2.8271e-01, -7.9888e-01,\n",
       "         1.1062e+00,  4.6667e-01,  2.5040e+00, -5.0132e-01,  5.8942e-01,\n",
       "         2.4908e-01, -4.9075e-01, -8.6088e-01, -5.4032e-02,  1.2750e+00,\n",
       "         3.7331e-01, -1.1620e+00,  4.7155e-03, -2.9604e-01, -3.4236e-01,\n",
       "        -4.3695e-01,  1.5472e-01, -5.3004e-02,  1.4021e+00,  4.6574e-01,\n",
       "        -1.9112e-01,  6.2127e-01, -5.9934e-01, -1.4941e-01, -5.3493e-01,\n",
       "         2.4627e-02,  6.3412e-01,  1.6706e+00,  4.3146e-02,  6.6929e-01,\n",
       "        -6.7244e-01,  6.9219e-01,  2.9934e-02,  2.9347e-01,  3.6864e-01,\n",
       "         8.9426e-01,  5.6283e-02, -3.5741e-01,  8.9381e-02, -1.4136e+00,\n",
       "        -4.9181e-01,  3.2873e-01, -5.2390e-01, -3.3620e-01,  4.4952e-01,\n",
       "        -1.8059e-01, -5.8400e-01,  1.0763e+00, -9.7750e-02,  7.3988e-01,\n",
       "         3.3248e-01,  4.3454e-02,  1.7525e-01,  2.2037e-01, -1.4335e-01,\n",
       "        -4.8352e-01,  6.2528e-01,  4.7547e-03, -1.6418e-01, -6.8599e-01,\n",
       "        -1.2886e-02,  2.3579e-01,  5.1027e-01, -8.4324e-02,  1.9178e-01,\n",
       "         2.1582e-01,  2.1841e+00, -2.0570e-01,  1.4139e-01,  2.4457e+00,\n",
       "        -4.4328e-01,  6.2814e-01,  4.0695e-01,  5.1243e-01, -3.8122e-01,\n",
       "         5.5976e-01, -7.7957e-02, -5.0273e-01,  5.9018e-02,  5.2933e-01,\n",
       "         5.3657e-01,  1.6380e-01, -7.6380e-02, -5.3196e-02,  4.7217e-02,\n",
       "        -1.4752e+00,  8.6445e-01, -2.1844e-01, -3.4345e-01, -3.1443e-01,\n",
       "        -2.3642e-01,  3.1615e-02, -2.4453e-02,  9.1187e-01, -1.9337e-01,\n",
       "         5.8253e-01, -3.1557e-01,  2.6214e-01,  1.7525e+00, -4.7159e-01,\n",
       "        -5.9733e-01, -8.9219e-01, -9.2374e-02,  7.3485e-01,  3.2334e-01,\n",
       "        -1.6766e+00, -4.5738e-01, -2.1092e-01, -7.6721e-01,  2.4080e-02,\n",
       "        -3.2697e-01, -1.3408e-01,  1.2251e-02, -1.7073e+00,  9.1963e-01,\n",
       "         6.1411e-01, -1.5626e-01, -1.6888e-01,  8.4162e-01, -9.5892e-01,\n",
       "        -1.0844e+00,  1.4881e-01,  1.9632e-01,  7.7409e-02,  1.4156e-01,\n",
       "        -2.2834e-01, -1.1843e-01, -1.2503e+00,  7.4304e-01,  1.4416e+00,\n",
       "        -3.3950e-01,  5.6264e-01,  1.9289e-03, -5.8882e-01, -6.8578e-01,\n",
       "         1.2650e-02,  2.2117e-01,  1.0095e-01,  1.9565e-02,  6.6394e-01,\n",
       "        -6.3752e-01, -1.6711e-01, -5.9285e-01, -4.7668e-01,  8.8474e-01,\n",
       "         6.1301e-01, -1.7508e-01, -7.1140e-01,  2.4005e-01,  1.1462e-01,\n",
       "        -7.4380e-01,  1.4700e+00, -1.9925e-01, -2.1531e-01, -1.2251e+00,\n",
       "         3.0770e-01,  6.8797e-01, -5.6631e-01,  8.2678e-01,  3.4808e-01,\n",
       "        -5.6226e-01, -9.9413e-01,  1.1029e+00, -1.1537e-01, -4.8309e-01,\n",
       "        -7.9505e-01, -8.9807e-02, -5.4381e-01,  1.0504e+00,  8.9512e-01,\n",
       "         4.9797e-01,  4.9560e-01,  1.1698e+00, -1.4465e-01,  6.1242e-01,\n",
       "        -2.3821e-01, -1.1307e+00, -6.5506e-01,  1.1397e+00,  4.8889e-01,\n",
       "        -1.2073e+00,  9.7565e-01, -3.8665e-01,  1.1495e+00,  1.9920e-01,\n",
       "         6.0732e-01,  7.3402e-01, -3.2823e-01,  6.6685e-02,  5.9845e-01,\n",
       "         1.1011e+00, -1.0330e+00,  4.2111e-01,  1.1999e-01,  4.6728e-02,\n",
       "         7.7463e-01, -6.1593e-01, -1.2715e+00,  4.0782e-01, -3.8996e-01,\n",
       "         7.0634e-02,  3.0282e-01, -1.6991e-01, -4.4998e-02,  1.1125e+00,\n",
       "        -1.1480e+00, -1.0587e+00, -4.2897e-01, -7.5486e-01, -1.8897e-01,\n",
       "         1.2163e+00, -7.0276e-01,  3.7787e-01, -2.8954e-01, -2.2572e-01,\n",
       "         1.4250e+00, -8.4101e-01,  4.6942e-01, -9.2195e-01, -6.1209e-01,\n",
       "         8.0839e-01, -5.8000e-01,  2.1487e-01,  3.3778e+00, -3.0348e-01,\n",
       "         1.4282e+00, -1.1809e+00, -1.2210e-01,  1.6974e-01, -2.3317e-01,\n",
       "        -1.0645e+00, -4.6249e-01, -3.8726e-01,  6.1198e-01, -1.7527e-01,\n",
       "         1.0208e+00,  1.0293e+00,  7.8348e-01,  4.5547e-02,  1.3896e+00,\n",
       "         2.4822e-01,  3.6393e-01,  1.4609e-01,  2.9588e-01, -1.6030e+00,\n",
       "        -1.7084e-01, -5.9474e-01,  1.2856e+00, -4.3874e-01,  2.1327e-01,\n",
       "        -6.1715e-01,  1.1980e+00, -6.1747e-01,  1.6338e-01,  1.3043e+00,\n",
       "         1.3634e-02,  8.0502e-01, -1.6922e-01, -6.4294e-01,  4.9836e-01,\n",
       "         2.6138e-01,  4.3368e-02,  2.5224e-01, -2.7753e-01,  4.3361e-01,\n",
       "        -8.1289e-01,  3.0330e-01, -4.4576e-01, -4.3434e-01, -8.3705e-01,\n",
       "        -3.1789e-01,  6.6209e-01, -9.6268e-01, -1.4616e+00,  6.2674e-01,\n",
       "        -5.8100e-01, -8.1699e-02, -1.7603e-01,  3.7135e-01, -4.4851e-01,\n",
       "        -1.5155e-01, -8.8182e-01,  1.9230e+00,  6.6994e-01, -8.8451e-01,\n",
       "        -9.7270e-01,  4.5804e-01,  1.9571e-01,  1.1892e+00,  1.1664e-01,\n",
       "         6.6026e-01,  1.9542e-01, -5.0468e-01,  3.4971e-01, -7.5621e-01,\n",
       "         7.9065e-01, -3.1511e-01, -1.7366e+00, -3.2171e-01, -7.0650e-01,\n",
       "        -1.0271e+00,  2.7688e-01, -6.5178e-01, -6.2081e-01,  7.5161e-01,\n",
       "        -2.1265e-01,  6.7958e-01, -2.4216e-01,  1.5087e-01,  2.8519e-01,\n",
       "        -6.0830e-01, -6.6821e-01, -1.7994e-01,  7.5535e-01, -1.2232e-01,\n",
       "        -1.6204e-01, -5.1012e-01, -3.3225e-01, -1.1444e+00,  9.6182e-01,\n",
       "         8.5121e-02,  1.0562e+00,  1.5191e+00, -2.2429e-01, -2.7118e-03,\n",
       "         3.6326e-01, -1.4854e-01, -6.3623e-02, -1.8995e-01,  1.0904e+00,\n",
       "         4.8412e-01, -2.3793e-01, -1.0808e-01,  2.8676e-01, -1.6041e-01,\n",
       "         8.9801e-01,  5.8945e-01,  7.3459e-01,  9.6137e-01,  1.6483e+00,\n",
       "         1.1829e-01,  1.5476e+00, -4.7707e-01, -4.3979e-01,  5.9378e-02,\n",
       "         4.7717e-01, -2.9899e-01,  5.3822e-01, -6.5502e-01,  5.2132e-01,\n",
       "         5.9853e-02,  6.4549e-01,  1.1591e+00, -1.8834e-01,  5.0871e-01,\n",
       "        -8.7178e-01,  3.9662e-01, -3.6634e-02, -2.7102e-01, -1.0376e+00,\n",
       "        -3.9104e-01,  9.2132e-01, -7.0803e-01, -9.6286e-01,  1.0882e+00,\n",
       "         3.0118e-01, -8.1267e-01,  3.8535e-01, -6.0028e-01,  3.0585e-01,\n",
       "         1.2196e+00, -1.0701e+00,  3.9756e-01,  9.4746e-01,  1.8190e+00,\n",
       "         3.6438e-01,  8.6656e-01, -1.2668e+00,  6.4570e-01,  6.4274e-01,\n",
       "         1.7152e+00, -8.8871e-01, -6.4040e-02,  7.8097e-01, -6.8617e-01,\n",
       "         1.5846e-01,  4.1687e-01,  4.4013e-01,  9.2187e-01,  2.4501e-01,\n",
       "         1.1721e+00,  1.0892e+00,  3.8711e-01,  6.1568e-01, -6.4919e-01,\n",
       "        -1.7959e-01, -2.4205e-01,  2.5268e-01,  6.5086e-01, -1.5588e-01,\n",
       "         4.9747e-02,  1.8400e+00,  1.1344e-01,  4.0495e-02, -1.8074e-01,\n",
       "         2.1394e+00,  7.3046e-01,  2.6881e-01,  2.0732e-02,  4.6137e-01,\n",
       "        -1.0741e+00,  7.9161e-01, -4.4372e-01,  1.0573e+00, -5.6358e-01,\n",
       "        -9.6237e-01,  5.8992e-01,  5.5276e-01, -5.5178e-01, -2.2338e-01,\n",
       "        -1.1385e+00, -1.1462e-01,  4.8217e-01, -2.7742e-02,  2.3937e-01,\n",
       "         1.8090e-01, -5.9373e-01,  9.1835e-02, -1.1463e+00,  1.7783e-01,\n",
       "         7.9076e-01,  1.3599e+00,  9.4021e-02, -6.2239e-01, -3.0779e-01,\n",
       "         2.0459e-01, -2.3920e-01,  1.2064e+00,  3.5463e-01,  2.7906e-01,\n",
       "        -4.5379e-01, -8.4819e-02, -6.0274e-01,  6.8782e-01,  2.7790e-02,\n",
       "         1.3533e+00,  1.8558e-01, -5.0861e-01,  4.0797e-01, -1.4091e+00,\n",
       "         1.0439e+00,  7.4485e-01, -6.5988e-02, -8.9791e-01, -6.6319e-01,\n",
       "         4.3678e-01, -3.1024e-01,  4.3888e-01,  1.6126e-01,  2.1729e-01,\n",
       "         6.2809e-01,  9.1303e-01,  1.2299e+00, -7.2054e-01,  3.5526e-01,\n",
       "        -1.1807e-01, -6.0173e-02, -6.2963e-01,  2.7142e-01, -6.5090e-01,\n",
       "         8.1006e-01, -1.2393e-01,  1.4478e+00,  3.2399e-01,  4.8201e-01,\n",
       "         1.1299e-01, -8.1906e-01, -9.7838e-01, -9.9647e-03,  3.6551e-01,\n",
       "        -2.4772e-01,  6.2413e-01,  5.6947e-02,  2.6786e-01,  3.3552e-01,\n",
       "         1.1783e-01,  3.6284e-03, -8.1315e-01,  2.0859e-01, -2.5847e-01,\n",
       "         4.9114e-01,  1.0965e+00, -2.8053e-01,  1.8367e-01, -2.4723e+00,\n",
       "         3.2128e-01,  7.7412e-01,  1.8395e-01,  6.7091e-01, -8.2781e-01,\n",
       "         3.2847e-01, -7.2773e-01,  8.3497e-01,  6.3675e-01, -1.6732e-01,\n",
       "        -4.9941e-01, -6.4598e-03,  1.3821e+00, -4.0015e-01,  8.6646e-02,\n",
       "         2.0658e-01,  4.6257e-01, -4.6218e-01, -1.0999e+00, -1.0173e+00,\n",
       "        -1.6799e-01, -1.2457e+00, -1.2064e-01,  2.7568e-01,  1.7410e+00,\n",
       "         1.4829e+00,  2.3641e-01, -1.7259e-02,  8.5032e-01,  3.2740e-01,\n",
       "         5.0344e-01, -7.6481e-02,  2.5152e-01,  6.1683e-01, -1.4734e-01,\n",
       "         6.9943e-01,  8.1998e-01, -5.8856e-01, -1.5573e+00, -3.4539e-01,\n",
       "        -2.1353e-03, -9.8463e-01,  4.6256e-03, -2.1918e-01,  2.2653e-01,\n",
       "         1.5436e+00, -3.4148e-01, -1.3844e+00,  7.1271e-01, -5.5610e-01,\n",
       "        -2.2625e-01, -4.0399e-01,  5.5933e-01,  1.0119e+00,  1.1700e+00,\n",
       "         1.1880e+00, -4.4626e-01, -1.3521e+00, -2.3402e-01, -3.3009e-01,\n",
       "         3.3759e-01,  4.9390e-01, -1.0302e+00,  7.5773e-01,  2.4563e-02,\n",
       "        -1.8915e+00,  8.8234e-01, -1.0214e+00,  1.1322e+00, -1.6862e+00,\n",
       "        -1.2719e+00, -9.0147e-01, -1.6515e-03, -1.4982e-01,  2.1852e-01,\n",
       "        -1.0359e-01,  8.9650e-01,  9.7352e-02], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.last_hidden_state[0][3]  #入力文内の4番目の単語「犬」の埋め込み表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT内部の処理\n",
    "* Transformer：BERTのアーキテクチャの一種\n",
    "     * 2017年に発表されたニューラル機械翻訳モデル\n",
    "     * 大きくEncoderとDecoderから構成されている\n",
    "     * Encoder = Embedding + Positional Encoding\n",
    "     * TransformerにおけるEncoderをBERTと呼ぶ\n",
    "\n",
    "##### BERTの構造     \n",
    "* BERT = BertEmbeddings(Embedding) + BertEncoder\n",
    "* BertEmbeddings内のPositional Encodingという部分で単語を分散表現に変換\n",
    "* この分散表現を12個のBertLayer(まとめてBertEncoderと呼ぶ)を使って少しずつ修正している\n",
    "\n",
    "##### BertEmbeddings\n",
    "* Position Embeddings：単語の位置(Position)に関する分散表現(Embeddings)\n",
    "* Segment Embeddings：1文目の単語か2文目の単語かを表す埋め込み表現のベクトル\n",
    "* BertEmbeddings = 単語の分散表現ベクトル + Position Embeddings + Segment Embeddings\n",
    "\n",
    "##### BertLayer\n",
    "* BertLayer：BertEmbeddingsが入力される部分\n",
    "* この入力がMulti-Head Attentionに渡され、単語の埋め込み表現列が出力される\n",
    "* ここで得られた出力に対して、残差接続を行い、その結果に対してLayer Normalizationの処理を行う  \n",
    "* 残差接続：Multi-Head Attentionの出力にMulti-Head Attentionの入力を足す処理  \n",
    "* Layer Normalization：ニューラルネットワークの表現力の維持、学習の安定化、汎化性能の向上等を行う操作→深層学習が成功しやすくなるらしい\n",
    "* その結果を線形変換し、再び残差接続とLayer Normalizationを行う\n",
    "\n",
    "##### Multi-Head Attention\n",
    "* Multi-Head Attention：Self-Attentionのモデルを並列で行っている構造\n",
    "* BERTの核となる部分\n",
    "* Self-Attention：ある1文の単語だけを使って計算された単語間の関連度スコア\n",
    "* Attention(注意機構)：入力オブジェクトの各所に処理のための重みを付け、それらを統合して何らかの処理をする機構\n",
    "* 入力単語(query)が学習された単語辞書内に無かったとしても、辞書内の単語のkeyとvalueに対して重みづけした情報を基に出力が可能となる\n",
    "* 重みづけの操作の中で、辞書内の単語のvalueが更新されていく..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* より詳しい説明は https://crystal-method.com/topics/transformer-2/ へ！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTによる文書分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 文章のラベルデータからBERTモデルを用いて文書分類を行うパッケージ\n",
    "* 最終的な出力は分類されたクラス！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 日本語版BERTであるcl-tohoku/bert-base-japanese-v2を学習させる\n",
    "* num_labelsで分類するラベルの数を指定(デフォルトはnum_labels=2で2クラス分類器である)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'cl-tohoku/bert-base-japanese-v2',\n",
    "    num_labels = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"\"\"\n",
    "私は犬が好きです。一般に動物が好きです。\n",
    "言葉が使える動物がいたら楽しいと思います。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* モデリングのために、文書dをBERTへの入力の形である単語id列に変換\n",
    "* 日本語の文書であるため、BertJapaneseTokenizerを用いて事前学習済みモデルを学習させる\n",
    "* xは文書dを単語id列に変換し、モデリングのために必要な形に変換したもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer\n",
    "tknz = BertJapaneseTokenizer.from_pretrained(\n",
    "    'cl-tohoku/bert-base-japanese-v2')\n",
    "x = tknz.encode(d)  #\n",
    "x = torch.LongTensor(x).unsqueeze(0)\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* logitsは文書がどのラベルに属する可能性が高いかを確率で示したlogit値\n",
    "* [バッチサイズ, ラベル数]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* logit値の出力\n",
    "* 今回は第3のラベルが最大値であるため、第3のラベルに分類されると推定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1634,  0.2196, -0.0378], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.logits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 最適化関数を設定する\n",
    "* 損失関数の値に応じて、SGD(確率的勾配降下法)によってパラメータ更新が可能\n",
    "* params：更新したいパラメータを渡す\n",
    "* lr：学習率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "opt = optim.SGD([{'params':model.parameters(), 'lr':0.01}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 文書dの正解ラベルを1とするとき、正解ラベルのデータをgaとする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga = torch.LongTensor([1]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 正解ラベルのデータgaを与えてモデリングすることで、最適化関数optによってパラメータ更新が可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x, labels=ga)  #学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 損失値loss\n",
    "* stepはパラメータ更新の処理\n",
    "* 以下の処理を繰り返すことで、学習の処理を行うことができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = y.loss\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 学習終了後のモデルを、mymodel.binとして保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mymodel.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 保存したモデルをロードするために、保存したモデルの定義を行う\n",
    "* configファイルを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('myconfig.pkl', 'bw') as fw:\n",
    "    pickle.dump(model.config, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 保存したモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('myconfig.pkl', 'br') as f:\n",
    "    myconfig = pickle.load(f)\n",
    "    mymodel = BertForSequenceClassification(config=myconfig)\n",
    "    mymodel.load_state_dict(torch.load('mymodel.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTによる系列ラベリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 系列ラベリング：文章をtokenizeし、それぞれのtokenにラベルを付与する方法→BertForTokenClassificationを利用\n",
    "* BertForTokenClassification→入力となるデータ系列の各データにラベルを付与するタスクに適している\n",
    "* 各tokenに対する各ラベルの確率からビタビアルゴリズムを利用して最終的なラベルを決定\n",
    "     * ビタビアルゴリズム：観測された事象系列を結果として生じる隠された状態の最も尤もらしい並びを探す動的計画法アルゴリズム(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "                 'cl-tohoku/bert-base-japanese-v2',\n",
    "                 num_labels = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"田中さんは茨城大学の学生です。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['田中', 'さん', 'は', '茨城', '大学', 'の', '学生', 'です', '。']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer\n",
    "tknz = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v2')\n",
    "tknz.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 文章sを単語id列に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 13026, 11689, 897, 14121, 11188, 896, 12229, 12461, 829, 3]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tknz.encode(s)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* モデリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor(x).unsqueeze(0)\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [バッチサイズ, 単語数, ラベル数]\n",
    "* 文章sは [CLS] 「田中」「さん」「は」「茨城」「大学」「の」「学生」「です」「。」 [SEP] の11単語から成る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 9])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 各単語に対する各ラベルのlogit値\n",
    "* y.logits [0] [i] [j] でi番目の単語に対するj番目のラベルのlogit値が得られる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.logits[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 最適化関数を設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "opt = optim.SGD([{'params':model.parameters(), 'lr':0.01}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ラベル名とラベルidの対応(たとえば、固有の組織名を表すラベルB-ORGのラベルidを3と設定)→テキスト参照\n",
    "* 正解のラベル列のデータgaを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga = torch.LongTensor([0, 1, 0, 0, 3, 7, 0, 0, 0, 0, 0]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 先程と同様に正解ラベル列のデータgaを与えたモデルを作成し、学習を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x, labels=ga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = y.loss\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelineによるタスクの推論\n",
    "* pipeline：各種タスクに対する事前学習済みモデルを利用して、そのタスクの推論処理を行ってくれるコマンド\n",
    "\n",
    "### 自然言語処理におけるタスクの種類\n",
    "* conversational：対話\n",
    "* feature-extraction：特徴抽出\n",
    "* fill-mask：マスク指定(隠す単語を指定して、推論を行わせる)\n",
    "* image-classification：画像識別\n",
    "* question-answering：質問応答\n",
    "* table-question-answering：表内容からの質問応答\n",
    "* text2text-generation：翻訳、要約、質問応答\n",
    "* text-classification / sentiment-analysis：評判分析\n",
    "* text-generation：テキスト生成\n",
    "* token-classification / ner：固有表現抽出\n",
    "* translation：翻訳\n",
    "* translation_xx_to_yy：XX_YY翻訳\n",
    "* summarization：要約\n",
    "* zero-shot-classification：Zero-shot分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評判分析\n",
    "* 入力文が肯定的か否定的かを判定するタスク\n",
    "* 文書分類と基本的に同じ\n",
    "* レビュー分析とかに使える？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "net = pipeline('text-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998513460159302}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'This book is very interesting.'\n",
    "net(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996337890625}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = 'This book has some interesting parts.'\n",
    "net(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* roberta-large-mnli：事前学習済みモデルRoBERTa-largeとデータセットMNLIを利用して学習されたモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "net2 = pipeline('text-classification', model='roberta-large-mnli')  #positive, neutral, negativeで判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEUTRAL', 'score': 0.6103535294532776}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* daigo/bert-base-japanese-sentiment：日本語の評判分析モデルの１つ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net3 = pipeline('text-classification',\n",
    "#                model='daigo/bert-base-japanese-sentiment')\n",
    "# net3(\"この犬は本当にお利口さんだ。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 固有表現抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99895644,\n",
       "  'index': 3,\n",
       "  'word': 'Tanaka',\n",
       "  'start': 3,\n",
       "  'end': 9},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9990582,\n",
       "  'index': 8,\n",
       "  'word': 'I',\n",
       "  'start': 26,\n",
       "  'end': 27},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.99033654,\n",
       "  'index': 9,\n",
       "  'word': '##bara',\n",
       "  'start': 27,\n",
       "  'end': 31},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9977101,\n",
       "  'index': 10,\n",
       "  'word': '##ki',\n",
       "  'start': 31,\n",
       "  'end': 33},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9953762,\n",
       "  'index': 11,\n",
       "  'word': 'University',\n",
       "  'start': 34,\n",
       "  'end': 44}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = pipeline('ner')\n",
    "text = 'Mr.Tanaka is a student at Ibaraki University.'\n",
    "net(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dslim/bert-base-NER：BERTとCoNLL-2003を使って学習されたモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net2 = pipeline('ner',model='dslim/bert-base-NER')\n",
    "# net2(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要約\n",
    "* デフォルトの事前学習済みモデル：sshleifer/distilbart-cnn-12-6\n",
    "* BART(テキスト生成に利用される事前学習済みモデル)とデータセットCNN/DailyMailDatasetを利用して学習されたモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' BERT is designed to pre-train deep bidirectional representations from unlabeled text by conditioning on both left and right context in all layers . The pre-trained BERT model can be fine-tuned with just one'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = pipeline('summarization')\n",
    "doc = \"\"\"We introduce a new language representation \n",
    "model called BERT, which stands for Bidirectional Encoder \n",
    "Representations from Transformers. Unlike recent language \n",
    "representation models, BERT is designed to pre-train deep \n",
    "bidirectional representations from unlabeled text by \n",
    "jointly conditioning on both left and right context in all \n",
    "layers. As a result, the pre-trained BERT model can be \n",
    "fine-tuned with just one additional output layer to create \n",
    "state-of-the-art models for a wide range of tasks, such as \n",
    "question answering and language inference, without \n",
    "substantial task-specific architecture modifications. BERT \n",
    "is conceptually simple and empirically powerful. It obtains \n",
    "new state-of-the-art results on eleven natural language \n",
    "processing tasks, including pushing the GLUE score to 80.5% \n",
    "(7.7% point absolute improvement), MultiNLI accuracy to \n",
    "86.7% (4.6% absolute improvement), SQuAD v1.1 question \n",
    "answering Test F1 to 93.2 (1.5 point absolute improvement) \n",
    "and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute \n",
    "improvement).\"\"\"\n",
    "net(doc, max_length=50, min_length=20)  #要約の長さ指定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 質問応答\n",
    "* デフォルトの事前学習済みモデル：distilbert-base-cased-distilled-squad\n",
    "* 事前学習済みモデルDistilBERTとデータセットSQuADを利用して学習されたモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.7516631484031677, 'start': 716, 'end': 722, 'answer': 'eleven'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = pipeline('question-answering')\n",
    "q = \"How many tasks did BERT get SOTA on?\"\n",
    "net(context=doc, question=q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキスト生成\n",
    "* デフォルトの事前学習済みモデル：GPT-2\n",
    "* 途中までの文章を与え、それらしい文を生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\snaka\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this paper, we propose a new hypothesis about the relationship between the amount of time each person spent in the family, by age, sex, occupation, and race, and their contribution to the average social life of children and adults, using a large'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = pipeline('text-generation')\n",
    "net(\"In this paper, we propose a new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot文書分類\n",
    "* デフォルトの事前学習済みモデル：facebook/bart-large-mnli\n",
    "* 訓練データを用いずに文書分類を行う\n",
    "* ラベル自体が言語で表現されていることを利用\n",
    "* ラベルが埋め込まれるベクトル空間と文書が埋め込まれるベクトル空間が同一であれば、ラベルと文書間の距離を測ることができる\n",
    "* 距離が最小のラベルに文書を分類する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Ichiro is the best baseball player ever.',\n",
       " 'labels': ['sports', 'economics', 'politics'],\n",
       " 'scores': [0.9978312849998474, 0.0013916805619373918, 0.0007770126685500145]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = pipeline('zero-shot-classification')\n",
    "doc = \"Ichiro is the best baseball player ever.\"\n",
    "labels = [\"politics\", \"economics\", \"sports\"]\n",
    "net(doc, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'My dog is really smart.',\n",
       " 'labels': ['positive', 'negative'],\n",
       " 'scores': [0.9315671920776367, 0.06843287497758865]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"My dog is really smart.\"\n",
    "labels = [\"positive\", \"negative\"]\n",
    "net(doc, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
